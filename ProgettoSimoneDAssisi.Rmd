---
title: "ProgettoSimoneDAssisi"
output: html_document
---

```{r}
#STUDIO ED ANALISI DI DATASET LEGATO AL PHISHING
#Author: Simone D'Assisi
```

```{r}
# INSTALLAZIONE PACCHETTI
packages <- c("corrplot", "tidyverse", "ggplot2", "DataExplorer", "e1071", "caTools", "caret", "moments")

install.packages(setdiff(packages, rownames(installed.packages())))

# Caricamento delle librerie
library(tidyverse)  # Per manipolazione e visualizzazione dei dati
library(corrplot)   # Per la visualizzazione delle correlazioni
library(ggplot2)    # Per la creazione di grafici
library(DataExplorer)  # Per report automatici sull'EDA
library(e1071)
library(reshape2)
library(caTools)
library(caret)
library(moments)

```

```{r}

#ANALISI DESCRITTIVA DEL DATASET

#FASE 1: In questa fase il Dataset viene caricato e viene mostrato se sono presenti valori mancanti

# Caricamento del dataset
phishing_data <- read.csv("Dataset/Phishing_URL_Dataset_2.csv", sep = ";")

# Visualizzare le prime righe del dataset
head(phishing_data)

# Verifica della struttura del dataset
str(phishing_data)

# Controllo dei valori mancanti
colSums(is.na(phishing_data))

```

```{r}

# FASE 2: vengono analizzate le feature che hanno un valore predittivo maggiore

# Codifica numerica delle colonne 'Domain' e 'Title'  per la creazione della matrice di correlazione
phishing_data$Domain_numeric <- as.numeric(factor(phishing_data$Domain))
phishing_data$Title_numeric <- as.numeric(factor(phishing_data$Title))

# CORRELAZIONE TRA TUTTE LE FEATURE E LA VARIABILE TARGET

# Esclusione delle variabili non numeriche
numeric_data <- phishing_data[sapply(phishing_data, is.numeric)]
correlation_matrix <- cor(numeric_data, use = "complete.obs")

label_correlation <- correlation_matrix["label", ]
sorted_label_correlation <- sort(label_correlation, decreasing = TRUE)
print(sorted_label_correlation)

# Selezione delle 5 feature più correlate con la variabile target
top_features <- names(sorted_label_correlation[1:6])

```

```{r}

# Esegui la selezione delle feature
top_features_data <- phishing_data[, top_features]

# FASE 3: Calcolo delle distribuzioni di Frequenza

for (feature in top_features) {
  if(feature != "URLSimilarityIndex"){
    # Calcolo la frequenza dei valori 0 e 1
    feature_counts <- table(phishing_data[[feature]])
    
    # Barplot per le feature binarie (0 e 1)
    barplot(feature_counts,
            main = paste("Distribuzione della Feature:", feature),
            col = c("#00AFBB", "#E7B800"),
            names.arg = names(feature_counts),
            xlab = feature,
            ylab = "Frequenza")
  
    # frequenza relativa
    feature_relative <- feature_counts / sum(feature_counts)
    
    print(feature)
    print("Frequenza Assoluta:")
    print(feature_counts)
    print("Frequenza Relativa:")
    print(feature_relative)
  }
}

# URLSimilarityIndex
bins <- seq(0, 100, by = 10)

phishing_data$URLSimilarityIndexClass <- cut(phishing_data$URLSimilarityIndex, 
                                             breaks = bins, 
                                             include.lowest = TRUE, 
                                             right = FALSE, 
                                             labels = paste0("[", bins[-length(bins)], "-", bins[-1], ")"))

# Calcola le frequenze per ciascun intervallo
class_counts <- table(phishing_data$URLSimilarityIndexClass)

# Crea un barplot per visualizzare la distribuzione delle classi di URLSimilarityIndex
barplot(class_counts,
        main = "Distribuzione di URLSimilarityIndex in classi",
        col = "#00AFBB",
        xlab = "Intervallo di URLSimilarityIndex",
        ylab = "Frequenza")  # 'las = 2' per ruotare le etichette dell'asse X

# Calcola le frequenze relative per ciascun intervallo
class_relative <- class_counts / sum(class_counts)

# Stampa le frequenze assolute e relative
print("Frequenze assolute per URLSimilarityIndex:")
print(class_counts)
print("Frequenze relative per URLSimilarityIndex:")
print(class_relative)

```

```{r}

# FASE 4: calcolo della Funzione di Distribuzione Empirica Continua per URLSimilarityIndex

# Calcola la frequenza cumulativa
cumulative_frequency <- cumsum(class_relative)

# Creiamo una lista per la FDEC, includendo i valori degli intervalli
FDEC_data <- data.frame(
  Intervallo = names(class_counts),
  FrequenzaAssoluta = class_counts,
  FrequenzaRelativa = class_relative,
  FrequenzaCumulativa = cumulative_frequency
)

# Visualizzare i risultati
print(FDEC_data)

# Visualizzare la FDEC come un grafico
plot(bins[-length(bins)], 
     cumulative_frequency, 
     type = "b", 
     main = "FDEC per URLSimilarityIndex",
     xlab = "Intervallo di URLSimilarityIndex", 
     ylab = "F(x)", 
     col = "#00AFBB", 
     lwd = 2)

```

```{r}

# FASE 5: calcolo degli indici di sintesi

# Indici per le feature binarie
for (feature in top_features) {
  if (feature != "URLSimilarityIndex") {
    # Calcolo la media
    media_bin <- mean(phishing_data[[feature]])
    
    # Calcolo la varianza
    var_bin <- var(phishing_data[[feature]])
    
    # Calcolo la deviazione standard
    sd_bin <- sd(phishing_data[[feature]])
    
    # Calcolo la moda
    moda_bin <- names(which.max(table(phishing_data[[feature]])))
    
    # Stampa dei risultati
    cat("\nFeature:", feature, "\n")
    cat("Media:", round(media_bin, 4), "\n")
    cat("Varianza:", round(var_bin, 4), "\n")
    cat("Deviaizone Standard:", round(sd_bin, 4), "\n")
    cat("Moda:", moda_bin, "\n")
  }
}

# Indici per URLSimilarityIndex
mean_url <- mean(phishing_data$URLSimilarityIndex)
median_url <- median(phishing_data$URLSimilarityIndex)
mode_url <- as.numeric(names(sort(table(phishing_data$URLSimilarityIndex), decreasing = TRUE)[1]))
var_url <- var(phishing_data$URLSimilarityIndex)
sd_url <- sd(phishing_data$URLSimilarityIndex)
min_url <- min(phishing_data$URLSimilarityIndex)
max_url <- max(phishing_data$URLSimilarityIndex)
quantiles_url <- quantile(phishing_data$URLSimilarityIndex, probs = c(0.25, 0.75))

# Stampa dei risultati
cat("\nIndice di sintesi per URLSimilarityIndex:\n")
cat("Media:", round(mean_url, 4), "\n")
cat("Mediana:", round(median_url, 4), "\n")
cat("Moda:", mode_url, "\n")
cat("Varianza:", round(var_url, 4), "\n")
cat("Deviazione Standard:", round(sd_url, 4), "\n")
cat("Valore Minimo:", min_url, "\n")
cat("Valore Massimo:", max_url, "\n")
cat("Primo Quartile (Q1):", quantiles_url[1], "\n")
cat("Terzo Quartile (Q3):", quantiles_url[2], "\n")

```

```{r}

#FASE 5.1: Calcolo quartili

quantiles_url <- summary(phishing_data$URLSimilarityIndex)

# Creazione del boxplot
boxplot(phishing_data$URLSimilarityIndex,
        main = "Boxplot di URLSimilarityIndex",
        ylab = "Valori di URLSimilarityIndex",
        col = "#00AFBB",
        border = "darkblue",
        notch = FALSE)

# Stampa dei risultati
cat("\nQuartili per URLSimilarityIndex:\n")
cat("Minimo (Q0): ", quantiles_url[1], "\n")
cat("Primo Quartile (Q1):", quantiles_url[2], "\n")
cat("Mediana (Q2):", quantiles_url[3], "\n")
cat("Terzo Quartile (Q3):", quantiles_url[4], "\n")
cat("Massimo (Q4):", quantiles_url[5], "\n")

```

```{r} 

# FASE 5.2: Calcolo delle della misure di simmetria per URLSimilarityIndex
skewness_url <- skewness(phishing_data$URLSimilarityIndex)
kurtosis_url <- kurtosis(phishing_data$URLSimilarityIndex)

cat("Skewness per URLSimilarityIndex:", round(skewness_url, 4), "\n")
cat("Curtosi per URLSimilarityIndex:", round(kurtosis_url, 4), "\n")

# Grafico per la skewness e la curtosi
ggplot(phishing_data, 
       aes(x = URLSimilarityIndex)) + 
  geom_histogram(
    aes(y = ..density..),
    bins = 10, 
    fill = "#00AFBB", 
    color = "black", 
    alpha = 0.7) + 
  geom_density(alpha = 0.2, fill = "red") + 
  labs(title = "Skewness e Curtosi di URLSimilarityIndex", 
       x = "URLSimilarityIndex",
       y = "Densità") + 
  theme_minimal() + 
  annotate("text", x = max(phishing_data$URLSimilarityIndex) * 0.8, 
           y = 0.02, 
           label = paste("Skewness =", round(skewness_url, 4)), 
           color = "red") +
  annotate("text", x = max(phishing_data$URLSimilarityIndex) * 0.8, 
           y = 0.015, 
           label = paste("Curtosi =", round(kurtosis_url, 4)), 
           color = "blue")

```

```{r}
#FASE 6: Statistica descrittiva bivariata

# Calcolo della covarianza tra variabili
for (feature in top_features) {
  if (feature != "URLSimilarityIndex") {
    # Covarianza tra la variabile continua e la binaria
    cov_bin_cont <- cov(phishing_data$URLSimilarityIndex, phishing_data[[feature]])
    cat("\nCovarianza tra", feature, " e URLSimilarityIndex:", round(cov_bin_cont, 4), "\n")
    
    # Covarianza tra due variabili binarie
    for (feature2 in top_features) {
      if (feature != feature2) {
        cov_bin_bin <- cov(phishing_data[[feature]], phishing_data[[feature2]])
        cat("Covarianza tra", feature, "e", feature2, ":", round(cov_bin_bin, 4), "\n")
      }
    }
  }
}


# Calcolo del coefficiente di correlazione tra la variabile target e tutte le altre
cat("\nCoefficiente di correlazione con la variabile target 'label':\n")
for (feature in top_features) {
  if (feature != "label") {
    cor_with_label <- cor(phishing_data$label, phishing_data[[feature]], method = "pearson")
    cat("Correlazione tra label e", feature, ":", round(cor_with_label, 4), "\n")
  }
}

# Sottocampionamento casuale dei dati
set.seed(123)  # Per riproducibilità
sampled_data <- phishing_data[sample(1:nrow(phishing_data), 100), ]

# Creazione degli scatterplot con la retta di regressione
for (feature in top_features) {
  if (feature != "label") {
    plot <- ggplot(sampled_data, aes_string(x = feature, y = "label")) +
      geom_jitter(width = 0.1, height = 0.1, alpha = 0.5, color = "#00AFBB") +  # Cambia il colore dei pallini
      geom_smooth(method = "lm", color = "red", se = FALSE) +  # Aggiungi la retta di regressione
      labs(title = paste("Scatterplot tra", feature, "e label"),
           x = feature, y = "label") +
      theme_minimal()
    print(plot)  # Mostra il grafico
  }
}

# Calcolo della matrice di correlazione
cor_matrix <- cor(phishing_data[, top_features], method = "pearson")

# Heatmap della matrice di correlazione con asse X verticale
cor_matrix_melted <- melt(cor_matrix)
ggplot(cor_matrix_melted, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(midpoint = 0, low = "blue", high = "red", mid = "white") +
  theme_minimal() +
  labs(title = "Matrice di Correlazione tra le variabili") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Rotazione dei nomi sull'asse X

```

```{r}

# FASE 6.1: Tabelle di Contingenza

cat("\n# Tabelle di contingenza\n")

# Creazione degli intervalli di 10 per URLSimilarityIndex
phishing_data$URLSimilarityIndex_bins <- cut(
  phishing_data$URLSimilarityIndex, 
  breaks = seq(0, 100, by = 10),
  include.lowest = TRUE, 
  right = FALSE
)

# Tabella di contingenza tra 'label' e URLSimilarityIndex
cat("\nTabella di contingenza tra 'label' e intervalli di URLSimilarityIndex:\n")
url_contingency_table <- table(phishing_data$label, phishing_data$URLSimilarityIndex_bins)
print(url_contingency_table)

# Tabelle di contingenza per le variabili binarie
for (feature in top_features) {
  if (feature != "label" && feature != "URLSimilarityIndex") {
    contingency_table <- table(phishing_data$label, phishing_data[[feature]])
    cat("\nTabella di contingenza tra 'label' e", feature, ":\n")
    print(contingency_table)
  }
}

```

```{r}

#FASE 7: Creazione del modello

#Per riproducibilità
set.seed(123)

# Dividere il dataset in 70% training e 30% test
split <- sample.split(top_features_data$label, SplitRatio = 0.7)
train_data <- subset(top_features_data, split == TRUE)
test_data <- subset(top_features_data, split == FALSE)

# Creare il modello di regressione logistica
model <- glm(label ~ URLSimilarityIndex + HasSocialNet + HasCopyrightInfo + HasDescription + IsHTTPS,
             data = train_data, 
             family = binomial)

# Riassunto del modello
summary(model)

# Fare previsioni sul test set
test_data$predicted_prob <- predict(model, newdata = test_data, type = "response")

# Convertire le probabilità in etichette binarie (threshold 0.5)
test_data$predicted_label <- ifelse(test_data$predicted_prob > 0.5, 1, 0)

# Calcolare l'accuratezza del modello
accuracy <- mean(test_data$predicted_label == test_data$label)
cat("Accuratezza del modello sul test set:", accuracy, "\n")

# Creare una matrice di confusione
conf_matrix <- confusionMatrix(as.factor(test_data$predicted_label), as.factor(test_data$label))
print(conf_matrix)




```

```{r}

# Fase 7.1: Confronto con un altro dataset identico

test_set <- read.csv("Dataset/Phishing_URL_Testset.csv", sep = ";")

# Prevedere le probabilità
test_set$predicted_prob <- predict(model, newdata = test_set, type = "response")

# Convertire le probabilità in etichette binarie (threshold 0.5)
test_set$predicted_label <- ifelse(test_set$predicted_prob > 0.5, 1, 0)

# Accuratezza
accuracy <- mean(test_set$predicted_label == test_set$label)
cat("Accuratezza del modello sul nuovo dataset:", accuracy, "\n")

# Confusion Matrix
conf_matrix <- confusionMatrix(as.factor(test_set$predicted_label), as.factor(test_set$label))
print(conf_matrix)

```

```{r}

# Estrazione righe come esempio per l'LLM

first_five_rows <- head(phishing_data, 5)

# Salvare le righe estratte in un nuovo file .csv
write.csv(first_five_rows, "First_Five_Rows.csv", row.names = FALSE)

cat("Il file 'First_Five_Rows.csv' è stato salvato con successo.")

```

```{r}
# FASE 8: Caricamento Dataset Sintetico

synthetic_data <- read.csv("Dataset/Synthetic_Phishing_Dataset.csv")

head(synthetic_data)

# Esegui la selezione delle feature
synthetic_top_data <- synthetic_data[, top_features]

```

```{r}

# FASE 8.1: Calcolo dei coeff. di correlazione con label

# Codifica numerica delle colonne 'Domain' e 'Title'  per la creazione della matrice di correlazione
synthetic_data$Domain_numeric <- as.numeric(factor(synthetic_data$Domain))
synthetic_data$Title_numeric <- as.numeric(factor(synthetic_data$Title))

# Esclusione delle variabili non numeriche
numeric_data <- synthetic_data[sapply(synthetic_data, is.numeric)]
correlation_matrix <- cor(numeric_data, use = "complete.obs")

label_correlation <- correlation_matrix["label", ]
sorted_label_correlation <- sort(label_correlation, decreasing = TRUE)
print(sorted_label_correlation)

```

```{r}

# FASE 8.2: Calcolo delle distribuzioni di Frequenza

for (feature in top_features) {
  if(feature != "URLSimilarityIndex"){
    # Calcolo la frequenza dei valori 0 e 1
    feature_counts <- table(synthetic_top_data[[feature]])
    
    # Barplot per le feature binarie (0 e 1)
    barplot(feature_counts,
            main = paste("Distribuzione della Feature:", feature),
            col = c("#00AFBB", "#E7B800"),
            names.arg = names(feature_counts),
            xlab = feature,
            ylab = "Frequenza")
  
    # frequenza relativa
    feature_relative <- feature_counts / sum(feature_counts)
    
    print(feature)
    print("Frequenza Assoluta:")
    print(feature_counts)
    print("Frequenza Relativa:")
    print(feature_relative)
  }
}

# URLSimilarityIndex
bins <- seq(0, 100, by = 10)

synthetic_top_data$URLSimilarityIndexClass <- cut(synthetic_top_data$URLSimilarityIndex, 
                                             breaks = bins, 
                                             include.lowest = TRUE, 
                                             right = FALSE, 
                                             labels = paste0("[", bins[-length(bins)], "-", bins[-1], ")"))

# Calcola le frequenze per ciascun intervallo
class_counts <- table(synthetic_top_data$URLSimilarityIndexClass)

# Crea un barplot per visualizzare la distribuzione delle classi di URLSimilarityIndex
barplot(class_counts,
        main = "Distribuzione di URLSimilarityIndex in classi",
        col = "#00AFBB",
        xlab = "Intervallo di URLSimilarityIndex",
        ylab = "Frequenza")  # 'las = 2' per ruotare le etichette dell'asse X

# Calcola le frequenze relative per ciascun intervallo
class_relative <- class_counts / sum(class_counts)

# Stampa le frequenze assolute e relative
print("Frequenze assolute per URLSimilarityIndex:")
print(class_counts)
print("Frequenze relative per URLSimilarityIndex:")
print(class_relative)

```

```{r}

# FASE 8.3: calcolo della Funzione di Distribuzione Empirica Continua per URLSimilarityIndex sintetica

# Calcola la frequenza cumulativa
cumulative_frequency <- cumsum(class_relative)

# Creiamo una lista per la FDEC, includendo i valori degli intervalli
FDEC_data <- data.frame(
  Intervallo = names(class_counts),
  FrequenzaAssoluta = class_counts,
  FrequenzaRelativa = class_relative,
  FrequenzaCumulativa = cumulative_frequency
)

# Visualizzare i risultati
print(FDEC_data)

# Visualizzare la FDEC come un grafico
plot(bins[-length(bins)], 
     cumulative_frequency, 
     type = "b", 
     main = "FDEC per URLSimilarityIndex",
     xlab = "Intervallo di URLSimilarityIndex", 
     ylab = "F(x)", 
     col = "#00AFBB", 
     lwd = 2)

```

```{r}

# FASE 8.4: calcolo degli indici di sintesi per le variabili sintetiche

# Indici per le feature binarie
for (feature in top_features) {
  if (feature != "URLSimilarityIndex") {
    # Calcolo la media
    media_bin <- mean(synthetic_top_data[[feature]])
    
    # Calcolo la varianza
    var_bin <- var(synthetic_top_data[[feature]])
    
    # Calcolo la deviazione standard
    sd_bin <- sd(synthetic_top_data[[feature]])
    
    # Calcolo la moda
    moda_bin <- names(which.max(table(synthetic_top_data[[feature]])))
    
    # Stampa dei risultati
    cat("\nFeature:", feature, "\n")
    cat("Media:", round(media_bin, 4), "\n")
    cat("Varianza:", round(var_bin, 4), "\n")
    cat("Deviaizone Standard:", round(sd_bin, 4), "\n")
    cat("Moda:", moda_bin, "\n")
  }
}

# Indici per URLSimilarityIndex
mean_url <- mean(synthetic_top_data$URLSimilarityIndex)
median_url <- median(synthetic_top_data$URLSimilarityIndex)
mode_url <- as.numeric(names(sort(table(synthetic_top_data$URLSimilarityIndex), decreasing = TRUE)[1]))
var_url <- var(synthetic_top_data$URLSimilarityIndex)
sd_url <- sd(synthetic_top_data$URLSimilarityIndex)
min_url <- min(synthetic_top_data$URLSimilarityIndex)
max_url <- max(synthetic_top_data$URLSimilarityIndex)
quantiles_url <- quantile(synthetic_top_data$URLSimilarityIndex, probs = c(0.25, 0.75))
IQR <- quantiles_url[2] - quantiles_url[1]
lim_inf <- quantiles_url[1] - (1.5 * IQR)
lim_sup <- quantiles_url[2] + (1.5 * IQR)

# Stampa dei risultati
cat("\nIndice di sintesi per URLSimilarityIndex:\n")
cat("Media:", round(mean_url, 4), "\n")
cat("Mediana:", round(median_url, 4), "\n")
cat("Moda:", mode_url, "\n")
cat("Varianza:", round(var_url, 4), "\n")
cat("Deviazione Standard:", round(sd_url, 4), "\n")

cat("\nQuartili:\n")
cat("Valore Minimo (Q0):", min_url, "\n")
cat("Primo Quartile (Q1):", quantiles_url[1], "\n")
cat("Terzo Quartile (Q3):", quantiles_url[2], "\n")
cat("Valore Massimo(Q4):", max_url, "\n")
cat("IQR:", IQR, "\n")
cat("Outlier: [", lim_inf, ",", lim_sup, "]\n")

# Creazione del boxplot
boxplot(synthetic_top_data$URLSimilarityIndex,
        main = "Boxplot di URLSimilarityIndex",
        ylab = "Valori di URLSimilarityIndex",
        col = "#00AFBB",
        border = "darkblue",
        notch = FALSE)


```

```{r} 

# FASE 8.5: Indici di simmetria per synURLSimilarityIndex
skewness_url <- skewness(synthetic_top_data$URLSimilarityIndex)
kurtosis_url <- kurtosis(synthetic_top_data$URLSimilarityIndex)

cat("Skewness per URLSimilarityIndex:", round(skewness_url, 4), "\n")
cat("Curtosi per URLSimilarityIndex:", round(kurtosis_url, 4), "\n")

# Grafico per la skewness e la curtosi
ggplot(synthetic_top_data, 
       aes(x = URLSimilarityIndex)) + 
  geom_histogram(
    aes(y = after_stat(density)),
    bins = 10, 
    fill = "#00AFBB", 
    color = "black", 
    alpha = 0.7) + 
  geom_density(alpha = 0.2, fill = "red") + 
  labs(title = "Skewness e Curtosi di URLSimilarityIndex", 
       x = "URLSimilarityIndex",
       y = "Densità") + 
  theme_minimal() + 
  annotate("text", x = max(synthetic_top_data$URLSimilarityIndex) * 0.8, 
           y = 0.02, 
           label = paste("Skewness =", round(skewness_url, 4)), 
           color = "red") +
  annotate("text", x = max(synthetic_top_data$URLSimilarityIndex) * 0.8, 
           y = 0.015, 
           label = paste("Curtosi =", round(kurtosis_url, 4)), 
           color = "blue")

```

```{r}
#FASE 8.6: Statistica descrittiva bivariata

# Calcolo della covarianza tra variabili
for (feature in top_features) {
  if (feature != "URLSimilarityIndex") {
    # Covarianza tra la variabile continua e la binaria
    cov_bin_cont <- cov(synthetic_top_data$URLSimilarityIndex, synthetic_top_data[[feature]])
    cat("\nCovarianza tra", feature, " e URLSimilarityIndex:", round(cov_bin_cont, 4), "\n")
    
    # Covarianza tra due variabili binarie
    for (feature2 in top_features) {
      if (feature != feature2) {
        cov_bin_bin <- cov(synthetic_top_data[[feature]], synthetic_top_data[[feature2]])
        cat("Covarianza tra", feature, "e", feature2, ":", round(cov_bin_bin, 4), "\n")
      }
    }
  }
}


# Calcolo del coefficiente di correlazione tra la variabile target e tutte le altre
cat("\nCoefficiente di correlazione con la variabile target 'label':\n")
for (feature in top_features) {
  if (feature != "label") {
    cor_with_label <- cor(synthetic_top_data$label, synthetic_top_data[[feature]], method = "pearson")
    cat("Correlazione tra label e", feature, ":", round(cor_with_label, 4), "\n")
  }
}

# Sottocampionamento casuale dei dati
set.seed(123)  # Per riproducibilità
sampled_data <- synthetic_top_data[sample(1:nrow(synthetic_top_data), 100), ]

# Creazione degli scatterplot con la retta di regressione
for (feature in top_features) {
  if (feature != "label") {
    plot <- ggplot(sampled_data, aes_string(x = feature, y = "label")) +
      geom_jitter(width = 0.1, height = 0.1, alpha = 0.5, color = "#00AFBB") +  # Cambia il colore dei pallini
      labs(title = paste("Scatterplot tra", feature, "e label"),
           x = feature, y = "label") +
      theme_minimal()
    print(plot)  # Mostra il grafico
  }
}

# Calcolo della matrice di correlazione
cor_matrix <- cor(synthetic_top_data[, top_features], method = "pearson")

# Heatmap della matrice di correlazione con asse X verticale
cor_matrix_melted <- melt(cor_matrix)
ggplot(cor_matrix_melted, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(midpoint = 0, low = "blue", high = "red", mid = "white") +
  theme_minimal() +
  labs(title = "Matrice di Correlazione tra le variabili") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  # Rotazione dei nomi sull'asse X

```
```{r}

# FASE 8.7: Tabelle di Contingenza delle variabili sintetiche

cat("\n# Tabelle di contingenza\n")

# Creazione degli intervalli di 10 per URLSimilarityIndex
synthetic_top_data$URLSimilarityIndex_bins <- cut(
  synthetic_top_data$URLSimilarityIndex, 
  breaks = seq(0, 100, by = 10),
  include.lowest = TRUE, 
  right = FALSE
)

# Tabella di contingenza tra 'label' e URLSimilarityIndex
cat("\nTabella di contingenza tra 'label' e intervalli di URLSimilarityIndex:\n")
url_contingency_table <- table(synthetic_top_data$label, synthetic_top_data$URLSimilarityIndex_bins)
print(url_contingency_table)

# Tabelle di contingenza per le variabili binarie
for (feature in top_features) {
  if (feature != "label" && feature != "URLSimilarityIndex") {
    contingency_table <- table(synthetic_top_data$label, synthetic_top_data[[feature]])
    cat("\nTabella di contingenza tra 'label' e", feature, ":\n")
    print(contingency_table)
  }
}

```

```{r}

# FASE 8.8: Applicazione del modello su dati sintetici

# Prevedere le probabilità
synthetic_top_data$predicted_prob <- predict(model, newdata = synthetic_top_data, type = "response")

# Convertire le probabilità in etichette binarie (threshold 0.5)
synthetic_top_data$predicted_label <- ifelse(synthetic_top_data$predicted_prob > 0.5, 1, 0)

# Accuratezza
accuracy <- mean(synthetic_top_data$predicted_label == synthetic_top_data$label)
cat("Accuratezza del modello sul nuovo dataset:", accuracy, "\n")

# Confusion Matrix
conf_matrix <- confusionMatrix(as.factor(synthetic_top_data$predicted_label), as.factor(synthetic_top_data$label))
print(conf_matrix)

```